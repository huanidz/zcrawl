"""
Module ch·ª©a class SinglePageCrawler - crawler ƒë∆°n gi·∫£n cho m·ªôt trang web.
K·∫ø th·ª´a t·ª´ BaseCrawler v√† cung c·∫•p ch·ª©c nƒÉng crawl trang ƒë∆°n.
"""

from typing import Optional

from loguru import logger

from ..browser.browser_config import BrowserConfig
from ..models.CrawlResult import CrawlResult
from .BaseCrawler import BaseCrawler


class SinglePageCrawler(BaseCrawler):
    """
    Crawler ƒë∆°n gi·∫£n cho vi·ªác l·∫•y n·ªôi dung c·ªßa m·ªôt trang web.
    K·∫ø th·ª´a t·ª´ BaseCrawler v√† implement ph∆∞∆°ng th·ª©c crawl().
    """

    def __init__(
        self,
        browser_config: Optional[BrowserConfig] = None,
        auto_start: bool = True,
        wait_for_selector: Optional[str] = None,
        wait_timeout: int = 30000,
    ):
        """
        Kh·ªüi t·∫°o SinglePageCrawler.

        Args:
            browser_config (Optional[BrowserConfig]): C·∫•u h√¨nh cho tr√¨nh duy·ªát
            auto_start (bool): T·ª± ƒë·ªông kh·ªüi ƒë·ªông tr√¨nh duy·ªát sau khi t·∫°o
            wait_for_selector (Optional[str]): CSS selector ƒë·ªÉ ch·ªù tr∆∞·ªõc khi l·∫•y n·ªôi dung
            wait_timeout (int): Timeout khi ch·ªù element (ms)
        """
        super().__init__(browser_config, auto_start)
        self.wait_for_selector = wait_for_selector
        self.wait_timeout = wait_timeout

    async def crawl(
        self,
        url: str,
        wait_until: str = "domcontentloaded",
        timeout: Optional[int] = None,
        screenshot_path: Optional[str] = None,
        **kwargs,
    ) -> CrawlResult:
        """
        Crawl m·ªôt trang web v√† tr·∫£ v·ªÅ n·ªôi dung HTML.

        Args:
            url (str): URL ƒë·ªÉ crawl
            wait_until (str): ƒêi·ªÅu ki·ªán ch·ªù ("domcontentloaded", "load", "networkidle")
            timeout (Optional[int]): Timeout t√πy ch·ªânh
            screenshot_path (Optional[str]): ƒê∆∞·ªùng d·∫´n l∆∞u screenshot (n·∫øu c√≥)
            **kwargs: Additional parameters

        Returns:
            CrawlResult: K·∫øt qu·∫£ crawl v·ªõi URL v√† HTML content

        Raises:
            RuntimeError: N·∫øu crawler ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông
            Exception: N·∫øu c√≥ l·ªói trong qu√° tr√¨nh crawl
        """
        if not self._is_started:
            raise RuntimeError("Crawler ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông. G·ªçi start() tr∆∞·ªõc.")

        logger.info(f"B·∫Øt ƒë·∫ßu crawl URL: {url}")

        try:
            # ƒêi·ªÅu h∆∞·ªõng ƒë·∫øn trang
            await self.navigate_to(url, wait_until=wait_until, timeout=timeout)

            # Ch·ªù cho selector c·ª• th·ªÉ n·∫øu c√≥
            if self.wait_for_selector:
                logger.debug(f"Ch·ªù selector: {self.wait_for_selector}")
                await self.wait_for_element(
                    self.wait_for_selector, timeout=self.wait_timeout
                )

            # L·∫•y HTML c·ªßa trang
            html_content = await self.get_page_html()

            # Ch·ª•p screenshot n·∫øu c√≥ y√™u c·∫ßu
            if screenshot_path:
                await self.take_screenshot(screenshot_path)

            # T·∫°o k·∫øt qu·∫£
            result = CrawlResult(url=url, raw_html=html_content)

            navigable_links = await self.extract_navigable_links(html_content, url)
            result.navigable_links = navigable_links
            logger.info(f"üëâ result: {result.model_dump_json(indent=2)}")

            # logger.info(
            #     f"ƒê√£ crawl th√†nh c√¥ng URL: {url}, HTML length: {len(html_content)}"
            # )
            return result

        except Exception as e:
            logger.error(f"L·ªói khi crawl URL {url}: {e}")
            raise
